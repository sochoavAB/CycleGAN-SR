{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e297a143-e29b-43c8-b4b5-33c90bb2e5bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version\n",
      "3.12.1 (tags/v3.12.1:2305ca5, Dec  7 2023, 22:03:25) [MSC v.1937 64 bit (AMD64)]\n",
      "PyTorch version: 2.2.1+cu118\n",
      "Epoch [1/1], Step [1/117], D_A_loss: 1.7988, D_B_loss: 1.0306, G_loss: 32.8086\n",
      "Epoch [1/1], Step [101/117], D_A_loss: 0.4688, D_B_loss: 0.4135, G_loss: 3.2834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Santiago\\AppData\\Local\\Temp\\ipykernel_35388\\574170710.py:302: UserWarning: \n",
      "The maximal number of iterations maxit (set to 20 by the program)\n",
      "allowed for finding a smoothing spline with fp=s has been reached: s\n",
      "too small.\n",
      "There is an approximation returned but the corresponding weighted sum\n",
      "of squared residuals does not satisfy the condition abs(fp-s)/s < tol.\n",
      "  fitresult = UnivariateSpline(xData, yData, s=0.01, k=3)\n",
      "C:\\Users\\Santiago\\AppData\\Local\\Temp\\ipykernel_35388\\574170710.py:302: UserWarning: \n",
      "A theoretically impossible result was found during the iteration\n",
      "process for finding a smoothing spline with fp = s: s too small.\n",
      "There is an approximation returned but the corresponding weighted sum\n",
      "of squared residuals does not satisfy the condition abs(fp-s)/s < tol.\n",
      "  fitresult = UnivariateSpline(xData, yData, s=0.01, k=3)\n",
      "C:\\Users\\Santiago\\AppData\\Local\\Temp\\ipykernel_35388\\574170710.py:334: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ..\\torch\\csrc\\utils\\tensor_new.cpp:278.)\n",
      "  cutoff_AtoB = torch.tensor([cutoff_metric(img) for img in X_fakeB]).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1]\n",
      "Train - PSNR A->B: 8.3369±1.7672, PSNR B->A: 11.5372±2.0123\n",
      "Train - SSIM A->B: 0.9792±0.0072, SSIM B->A: 0.9895±0.0043\n",
      "Train - Cutoff A->B: 25.9896±0.4121, Cutoff B->A: 27.3916±0.6080\n",
      "Valid - PSNR A->B: 17.2726±nan, PSNR B->A: 11.7358±nan\n",
      "Valid - SSIM A->B: 0.9983±nan, SSIM B->A: 0.9911±nan\n",
      "Valid - Cutoff A->B: 26.6464±nan, Cutoff B->A: 27.3737±nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Santiago\\AppData\\Local\\Temp\\ipykernel_35388\\574170710.py:201: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at ..\\aten\\src\\ATen\\native\\ReduceOps.cpp:1760.)\n",
      "  return (psnr_AtoB.mean().item(), psnr_AtoB.std().item(), mse_AtoB.mean().item(), mse_AtoB.std().item(),\n",
      "C:\\Users\\Santiago\\AppData\\Local\\Temp\\ipykernel_35388\\574170710.py:202: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at ..\\aten\\src\\ATen\\native\\ReduceOps.cpp:1760.)\n",
      "  psnr_BtoA.mean().item(), psnr_BtoA.std().item(), mse_BtoA.mean().item(), mse_BtoA.std().item())\n",
      "C:\\Users\\Santiago\\AppData\\Local\\Temp\\ipykernel_35388\\574170710.py:262: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at ..\\aten\\src\\ATen\\native\\ReduceOps.cpp:1760.)\n",
      "  return (ssim_AtoB.mean().item(), ssim_AtoB.std().item(),\n",
      "C:\\Users\\Santiago\\AppData\\Local\\Temp\\ipykernel_35388\\574170710.py:263: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at ..\\aten\\src\\ATen\\native\\ReduceOps.cpp:1760.)\n",
      "  ssim_BtoA.mean().item(), ssim_BtoA.std().item())\n",
      "C:\\Users\\Santiago\\AppData\\Local\\Temp\\ipykernel_35388\\574170710.py:337: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at ..\\aten\\src\\ATen\\native\\ReduceOps.cpp:1760.)\n",
      "  return (cutoff_AtoB.mean().item(), cutoff_AtoB.std().item(),\n",
      "C:\\Users\\Santiago\\AppData\\Local\\Temp\\ipykernel_35388\\574170710.py:338: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at ..\\aten\\src\\ATen\\native\\ReduceOps.cpp:1760.)\n",
      "  cutoff_BtoA.mean().item(), cutoff_BtoA.std().item())\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import fftpack\n",
    "from scipy.interpolate import UnivariateSpline\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "print(\"Python version\")\n",
    "print(sys.version)\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "\n",
    "# Define the generator\n",
    "class ResnetBlock(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super(ResnetBlock, self).__init__()\n",
    "        self.conv_block = nn.Sequential(\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(dim, dim, kernel_size=3),\n",
    "            nn.InstanceNorm2d(dim),\n",
    "            nn.ReLU(True),\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(dim, dim, kernel_size=3),\n",
    "            nn.InstanceNorm2d(dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.conv_block(x)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_nc, output_nc, ngf=64, n_blocks=9):\n",
    "        super(Generator, self).__init__()\n",
    "        model = [\n",
    "            nn.ReflectionPad2d(3),\n",
    "            nn.Conv2d(input_nc, ngf, kernel_size=7, padding=0),\n",
    "            nn.InstanceNorm2d(ngf),\n",
    "            nn.ReLU(True)\n",
    "        ]\n",
    "\n",
    "        # Downsampling\n",
    "        n_downsampling = 2\n",
    "        for i in range(n_downsampling):\n",
    "            mult = 2**i\n",
    "            model += [\n",
    "                nn.Conv2d(ngf * mult, ngf * mult * 2, kernel_size=3, stride=2, padding=1),\n",
    "                nn.InstanceNorm2d(ngf * mult * 2),\n",
    "                nn.ReLU(True)\n",
    "            ]\n",
    "\n",
    "        # Resnet blocks\n",
    "        mult = 2**n_downsampling\n",
    "        for i in range(n_blocks):\n",
    "            model += [ResnetBlock(ngf * mult)]\n",
    "\n",
    "        # Upsampling\n",
    "        for i in range(n_downsampling):\n",
    "            mult = 2**(n_downsampling - i)\n",
    "            model += [\n",
    "                nn.ConvTranspose2d(ngf * mult, int(ngf * mult / 2), kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "                nn.InstanceNorm2d(int(ngf * mult / 2)),\n",
    "                nn.ReLU(True)\n",
    "            ]\n",
    "\n",
    "        model += [\n",
    "            nn.ReflectionPad2d(3),\n",
    "            nn.Conv2d(ngf, output_nc, kernel_size=7, padding=0),\n",
    "            nn.Tanh()\n",
    "        ]\n",
    "\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Define the discriminator\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_nc, ndf=64):\n",
    "        super(Discriminator, self).__init__()\n",
    "        model = [\n",
    "            nn.Conv2d(input_nc, ndf, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, True)\n",
    "        ]\n",
    "\n",
    "        model += [\n",
    "            nn.Conv2d(ndf, ndf * 2, kernel_size=4, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, True)\n",
    "        ]\n",
    "\n",
    "        model += [\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, kernel_size=4, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, True)\n",
    "        ]\n",
    "\n",
    "        model += [\n",
    "            nn.Conv2d(ndf * 4, ndf * 8, kernel_size=4, stride=1, padding=1),\n",
    "            nn.InstanceNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2, True)\n",
    "        ]\n",
    "\n",
    "        model += [nn.Conv2d(ndf * 8, 1, kernel_size=4, stride=1, padding=1)]\n",
    "\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Define the dataset\n",
    "class CycleGANDataset(Dataset):\n",
    "    def __init__(self, npz_file, transform=None, fraction=0.03):\n",
    "        data = np.load(npz_file)\n",
    "        total_samples = len(data['arr_0'])\n",
    "        num_samples = int(total_samples * fraction)\n",
    "        \n",
    "        self.A = data['arr_0'][:num_samples]\n",
    "        self.B = data['arr_1'][:num_samples]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.A)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        A = self.A[idx].astype(np.float32) / 255.0  # Normalize to [0, 1]\n",
    "        B = self.B[idx].astype(np.float32) / 255.0  # Normalize to [0, 1]\n",
    "        \n",
    "        A = torch.from_numpy(A).permute(2, 0, 1)  # Change from (H, W, C) to (C, H, W)\n",
    "        B = torch.from_numpy(B).permute(2, 0, 1)  # Change from (H, W, C) to (C, H, W)\n",
    "        \n",
    "        if self.transform:\n",
    "            A = self.transform(A)\n",
    "            B = self.transform(B)\n",
    "        \n",
    "        return A, B\n",
    "# Define the loss functions\n",
    "class GANLoss(nn.Module):\n",
    "    def __init__(self, use_lsgan=True, target_real_label=1.0, target_fake_label=0.0):\n",
    "        super(GANLoss, self).__init__()\n",
    "        self.register_buffer('real_label', torch.tensor(target_real_label))\n",
    "        self.register_buffer('fake_label', torch.tensor(target_fake_label))\n",
    "        if use_lsgan:\n",
    "            self.loss = nn.MSELoss()\n",
    "        else:\n",
    "            self.loss = nn.BCELoss()\n",
    "\n",
    "    def get_target_tensor(self, input, target_is_real):\n",
    "        if target_is_real:\n",
    "            target_tensor = self.real_label\n",
    "        else:\n",
    "            target_tensor = self.fake_label\n",
    "        return target_tensor.expand_as(input)\n",
    "\n",
    "    def __call__(self, input, target_is_real):\n",
    "        target_tensor = self.get_target_tensor(input, target_is_real)\n",
    "        return self.loss(input, target_tensor)\n",
    "    \n",
    "    # Metrics functions\n",
    "def generate_fake_samples(g_model, samples, device):\n",
    "    g_model = g_model.to(device)\n",
    "    samples = samples.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        X = g_model(samples)\n",
    "    return X\n",
    "\n",
    "\n",
    "def generate_real_samples(dataset, n_samples, device):\n",
    "    indices = torch.randint(0, len(dataset), (n_samples,))\n",
    "    samples = [dataset[i] for i in indices]\n",
    "    X1 = torch.stack([s[0] for s in samples])\n",
    "    X2 = torch.stack([s[1] for s in samples])\n",
    "    return X1.to(device), X2.to(device)\n",
    "\n",
    "def psnr(g_model_AtoB, g_model_BtoA, dataset, n_samples=15, device=None):\n",
    "    if device is None:\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    dataloader = DataLoader(dataset, batch_size=n_samples, shuffle=True)\n",
    "    X_realA, X_realB = next(iter(dataloader))\n",
    "    X_realA, X_realB = X_realA.to(device), X_realB.to(device)\n",
    "\n",
    "    # Ensure models are on the correct device\n",
    "    g_model_AtoB = g_model_AtoB.to(device)\n",
    "    g_model_BtoA = g_model_BtoA.to(device)\n",
    "    \n",
    "    X_fakeB = generate_fake_samples(g_model_AtoB, X_realA, device)\n",
    "    X_fakeA = generate_fake_samples(g_model_BtoA, X_realB, device)\n",
    "\n",
    "    # Proceed with the PSNR calculation\n",
    "    mse_AtoB = nn.functional.mse_loss(X_fakeB, X_realB, reduction='none').mean(dim=[1,2,3])\n",
    "    psnr_AtoB = 20 * torch.log10(2.0 / torch.sqrt(mse_AtoB))\n",
    "\n",
    "    mse_BtoA = nn.functional.mse_loss(X_fakeA, X_realA, reduction='none').mean(dim=[1,2,3])\n",
    "    psnr_BtoA = 20 * torch.log10(2.0 / torch.sqrt(mse_BtoA))\n",
    "\n",
    "    return (psnr_AtoB.mean().item(), psnr_AtoB.std().item(), mse_AtoB.mean().item(), mse_AtoB.std().item(),\n",
    "            psnr_BtoA.mean().item(), psnr_BtoA.std().item(), mse_BtoA.mean().item(), mse_BtoA.std().item())\n",
    "\n",
    "\n",
    "\n",
    "def structural_similarity(img1, img2):\n",
    "    C1 = (0.01 * 255)**2\n",
    "    C2 = (0.03 * 255)**2\n",
    "\n",
    "    # Las imágenes ya están en formato (batch, channels, height, width)\n",
    "    # y tienen 3 canales (RGB), entonces ajustamos el kernel a 3 canales.\n",
    "    \n",
    "    # Cambiar el kernel para que sea aplicado a cada canal por separado\n",
    "    kernel = torch.ones(3, 1, 11, 11) / 121  # Kernel de 3 canales\n",
    "    kernel = kernel.to(img1.device)\n",
    "\n",
    "    # Aplicamos convoluciones canal por canal con groups=3\n",
    "    mu1 = nn.functional.conv2d(img1, kernel, padding=5, groups=3)  # groups=3 para cada canal (RGB)\n",
    "    mu2 = nn.functional.conv2d(img2, kernel, padding=5, groups=3)\n",
    "\n",
    "    mu1_sq = mu1.pow(2)\n",
    "    mu2_sq = mu2.pow(2)\n",
    "    mu1_mu2 = mu1 * mu2\n",
    "\n",
    "    sigma1_sq = nn.functional.conv2d(img1 * img1, kernel, padding=5, groups=3) - mu1_sq\n",
    "    sigma2_sq = nn.functional.conv2d(img2 * img2, kernel, padding=5, groups=3) - mu2_sq\n",
    "    sigma12 = nn.functional.conv2d(img1 * img2, kernel, padding=5, groups=3) - mu1_mu2\n",
    "\n",
    "    # Calcular el mapa SSIM\n",
    "    ssim_map = ((2 * mu1_mu2 + C1) * (2 * sigma12 + C2)) / ((mu1_sq + mu2_sq + C1) * (sigma1_sq + sigma2_sq + C2))\n",
    "\n",
    "    # Retornar el valor promedio del SSIM por batch\n",
    "    return ssim_map.mean([1, 2, 3])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def avg_SSIM(g_model_AtoB, g_model_BtoA, dataset, n_samples=15, device=None):\n",
    "    if device is None:\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    dataloader = DataLoader(dataset, batch_size=n_samples, shuffle=True)\n",
    "    X_realA, X_realB = next(iter(dataloader))\n",
    "    X_realA, X_realB = X_realA.to(device), X_realB.to(device)\n",
    "\n",
    "    g_model_AtoB = g_model_AtoB.to(device)\n",
    "    g_model_BtoA = g_model_BtoA.to(device)\n",
    "    \n",
    "    X_fakeB = generate_fake_samples(g_model_AtoB, X_realA, device)\n",
    "    X_fakeA = generate_fake_samples(g_model_BtoA, X_realB, device)\n",
    "\n",
    "    # Convert from [-1, 1] to [0, 1] for SSIM calculation\n",
    "    X_realA = (X_realA + 1) / 2\n",
    "    X_realB = (X_realB + 1) / 2\n",
    "    X_fakeA = (X_fakeA + 1) / 2\n",
    "    X_fakeB = (X_fakeB + 1) / 2\n",
    "\n",
    "    ssim_AtoB = structural_similarity(X_fakeB, X_realB)\n",
    "    ssim_BtoA = structural_similarity(X_fakeA, X_realA)\n",
    "\n",
    "    return (ssim_AtoB.mean().item(), ssim_AtoB.std().item(),\n",
    "            ssim_BtoA.mean().item(), ssim_BtoA.std().item())\n",
    "def cutoff_metric(image):\n",
    "    # Convert PyTorch tensor to numpy array\n",
    "    image_np = image.squeeze().cpu().numpy()\n",
    "\n",
    "    # If the image has more than two dimensions, select only the spatial dimensions\n",
    "    if len(image_np.shape) == 3:  # (channels, height, width)\n",
    "        # Select only one channel (e.g., channel 0)\n",
    "        image_np = image_np[0]\n",
    "\n",
    "    # Ensure the image is in [0, 255] range for FFT calculation\n",
    "    image_np = (image_np * 127.5 + 127.5).astype(np.uint8)\n",
    "\n",
    "    # Compute the 2-D FFT and shift the zero-frequency component to the center of the array\n",
    "    fft = fftpack.fftshift(fftpack.fft2(image_np))\n",
    "\n",
    "    # Compute the magnitude spectrum\n",
    "    magnitude_spectrum = np.abs(fft)\n",
    "\n",
    "    # Determine the size of the magnitude spectrum (height, width)\n",
    "    Nx, Ny = magnitude_spectrum.shape\n",
    "    x0, y0 = Nx // 2, Ny // 2\n",
    "\n",
    "    # Define a set of radii to crop the magnitude spectrum\n",
    "    radius = np.arange(1, Nx, 5)\n",
    "\n",
    "    # Compute distances from center point to all pixels\n",
    "    y, x = np.ogrid[-y0:Ny-y0, -x0:Nx-x0]\n",
    "    distances = np.sqrt(x*x + y*y)\n",
    "\n",
    "    # Create mask and compute profile\n",
    "    ft_profile = np.array([np.sum(magnitude_spectrum[(distances >= r) & (distances < r+5)]) for r in radius[:-1]])\n",
    "\n",
    "    # Compute cutoff energy and cumulative energy\n",
    "    cutoff_energy = 0.95 * np.sum(ft_profile)\n",
    "    energy = np.cumsum(ft_profile)\n",
    "\n",
    "    # Fit smoothing spline to the cumulative energy data\n",
    "    xData, yData = np.arange(len(energy)), energy\n",
    "    fitresult = UnivariateSpline(xData, yData, s=0.01, k=3)\n",
    "\n",
    "    # Evaluate the fitted curve and find cutoff frequency\n",
    "    vect_frequency = np.arange(1, 300, 0.1)\n",
    "    a = fitresult(vect_frequency)\n",
    "    n = np.argwhere(a > cutoff_energy)[0]\n",
    "\n",
    "    # Estimate the cutoff frequency using linear interpolation\n",
    "    y1, y0, y = a[n], a[n-1], cutoff_energy\n",
    "    x0, x1 = vect_frequency[n-1], vect_frequency[n]\n",
    "    cutoff_freq = x0 + (y-y0)/(y1-y0)*(x1-x0)\n",
    "\n",
    "    return cutoff_freq\n",
    "\n",
    "def cutoff_batch(g_model_AtoB, g_model_BtoA, dataset, n_samples=15, device=None):\n",
    "    if device is None:\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    dataloader = DataLoader(dataset, batch_size=n_samples, shuffle=True)\n",
    "    X_realA, X_realB = next(iter(dataloader))\n",
    "    X_realA, X_realB = X_realA.to(device), X_realB.to(device)\n",
    "\n",
    "    g_model_AtoB = g_model_AtoB.to(device)\n",
    "    g_model_BtoA = g_model_BtoA.to(device)\n",
    "    \n",
    "    X_fakeB = generate_fake_samples(g_model_AtoB, X_realA, device)\n",
    "    X_fakeA = generate_fake_samples(g_model_BtoA, X_realB, device)\n",
    "\n",
    "    # Convert from [-1, 1] to [0, 1] for cutoff metric calculation\n",
    "    X_fakeA = (X_fakeA + 1) / 2\n",
    "    X_fakeB = (X_fakeB + 1) / 2\n",
    "    \n",
    "    cutoff_AtoB = torch.tensor([cutoff_metric(img) for img in X_fakeB]).to(device)\n",
    "    cutoff_BtoA = torch.tensor([cutoff_metric(img) for img in X_fakeA]).to(device)\n",
    "    \n",
    "    return (cutoff_AtoB.mean().item(), cutoff_AtoB.std().item(),\n",
    "            cutoff_BtoA.mean().item(), cutoff_BtoA.std().item())\n",
    "\n",
    "\n",
    "\n",
    "# Define the training function\n",
    "def train(netG_A2B, netG_B2A, netD_A, netD_B, train_loader, valid_loader, valid_paired_loader, num_epochs, device):\n",
    "    criterionGAN = GANLoss().to(device)\n",
    "    criterionCycle = nn.L1Loss()\n",
    "    criterionIdt = nn.L1Loss()\n",
    "\n",
    "    optimizer_G = optim.Adam(list(netG_A2B.parameters()) + list(netG_B2A.parameters()), lr=0.0002, betas=(0.5, 0.999))\n",
    "    optimizer_D_A = optim.Adam(netD_A.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "    optimizer_D_B = optim.Adam(netD_B.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "    # Learning rate scheduler\n",
    "    scheduler_G = optim.lr_scheduler.StepLR(optimizer_G, step_size=30, gamma=0.1)\n",
    "    scheduler_D_A = optim.lr_scheduler.StepLR(optimizer_D_A, step_size=30, gamma=0.1)\n",
    "    scheduler_D_B = optim.lr_scheduler.StepLR(optimizer_D_B, step_size=30, gamma=0.1)\n",
    "\n",
    "    train_metrics = []\n",
    "    valid_metrics = []\n",
    "    losses = {'G': [], 'D_A': [], 'D_B': []}\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_losses = {'G': [], 'D_A': [], 'D_B': []}\n",
    "        for i, (real_A, real_B) in enumerate(train_loader):\n",
    "            real_A = real_A.to(device)\n",
    "            real_B = real_B.to(device)\n",
    "\n",
    "            # Train Generators\n",
    "            optimizer_G.zero_grad()\n",
    "\n",
    "            fake_B = netG_A2B(real_A)\n",
    "            fake_A = netG_B2A(real_B)\n",
    "\n",
    "            loss_idt_A = criterionIdt(netG_A2B(real_B), real_B) * 5.0\n",
    "            loss_idt_B = criterionIdt(netG_B2A(real_A), real_A) * 5.0\n",
    "\n",
    "            loss_GAN_A2B = criterionGAN(netD_B(fake_B), True)\n",
    "            loss_GAN_B2A = criterionGAN(netD_A(fake_A), True)\n",
    "\n",
    "            recovered_A = netG_B2A(fake_B)\n",
    "            loss_cycle_ABA = criterionCycle(recovered_A, real_A) * 10.0\n",
    "\n",
    "            recovered_B = netG_A2B(fake_A)\n",
    "            loss_cycle_BAB = criterionCycle(recovered_B, real_B) * 10.0\n",
    "\n",
    "            loss_G = loss_GAN_A2B + loss_GAN_B2A + loss_cycle_ABA + loss_cycle_BAB + loss_idt_A + loss_idt_B\n",
    "            loss_G.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "            # Train Discriminator A\n",
    "            optimizer_D_A.zero_grad()\n",
    "            loss_D_A = criterionGAN(netD_A(real_A), True) + criterionGAN(netD_A(fake_A.detach()), False)\n",
    "            loss_D_A.backward()\n",
    "            optimizer_D_A.step()\n",
    "\n",
    "            # Train Discriminator B\n",
    "            optimizer_D_B.zero_grad()\n",
    "            loss_D_B = criterionGAN(netD_B(real_B), True) + criterionGAN(netD_B(fake_B.detach()), False)\n",
    "            loss_D_B.backward()\n",
    "            optimizer_D_B.step()\n",
    "\n",
    "            epoch_losses['G'].append(loss_G.item())\n",
    "            epoch_losses['D_A'].append(loss_D_A.item())\n",
    "            epoch_losses['D_B'].append(loss_D_B.item())\n",
    "\n",
    "            if i % 100 == 0:\n",
    "                print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], '\n",
    "                      f'D_A_loss: {loss_D_A.item():.4f}, D_B_loss: {loss_D_B.item():.4f}, '\n",
    "                      f'G_loss: {loss_G.item():.4f}')\n",
    "\n",
    "        # Step the schedulers\n",
    "        scheduler_G.step()\n",
    "        scheduler_D_A.step()\n",
    "        scheduler_D_B.step()\n",
    "\n",
    "        for key in losses.keys():\n",
    "            losses[key].append(np.mean(epoch_losses[key]))\n",
    "\n",
    "        # Compute metrics\n",
    "        try:\n",
    "            train_metrics.append(compute_metrics(netG_A2B, netG_B2A, train_loader.dataset, device, n_samples=100))\n",
    "            valid_metrics.append(compute_metrics(netG_A2B, netG_B2A, valid_paired_loader.dataset, device, n_samples=100))\n",
    "        except RuntimeError as e:\n",
    "            print(f\"Error computing metrics: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Save metrics\n",
    "        save_metrics(train_metrics[-1], valid_metrics[-1], epoch)\n",
    "\n",
    "        # Save models every 20 epochs\n",
    "        if (epoch + 1) % 20 == 0:\n",
    "            save_models(netG_A2B, netG_B2A, epoch)\n",
    "\n",
    "        print_metrics(train_metrics[-1], valid_metrics[-1], epoch, num_epochs)\n",
    "\n",
    "    return train_metrics, valid_metrics, losses\n",
    "\n",
    "def compute_metrics(netG_A2B, netG_B2A, dataset, device, n_samples=100):\n",
    "    psnr_values = psnr(netG_A2B, netG_B2A, dataset, n_samples, device)\n",
    "    ssim_values = avg_SSIM(netG_A2B, netG_B2A, dataset, n_samples, device)\n",
    "    cutoff_values = cutoff_batch(netG_A2B, netG_B2A, dataset, n_samples, device)\n",
    "    \n",
    "    return {\n",
    "        'psnr': psnr_values,\n",
    "        'ssim': ssim_values,\n",
    "        'cutoff': cutoff_values\n",
    "    }\n",
    "\n",
    "def save_metrics(train_metrics, valid_metrics, epoch):\n",
    "    with open(f'metrics_epoch_{epoch+1}.txt', 'w') as f:\n",
    "        f.write(f\"Epoch {epoch+1}\\n\")\n",
    "        f.write(f\"Train - PSNR A->B: {train_metrics['psnr'][0]:.4f}±{train_metrics['psnr'][1]:.4f}, PSNR B->A: {train_metrics['psnr'][4]:.4f}±{train_metrics['psnr'][5]:.4f}\\n\")\n",
    "        f.write(f\"Train - SSIM A->B: {train_metrics['ssim'][0]:.4f}±{train_metrics['ssim'][1]:.4f}, SSIM B->A: {train_metrics['ssim'][2]:.4f}±{train_metrics['ssim'][3]:.4f}\\n\")\n",
    "        f.write(f\"Train - Cutoff A->B: {train_metrics['cutoff'][0]:.4f}±{train_metrics['cutoff'][1]:.4f}, Cutoff B->A: {train_metrics['cutoff'][2]:.4f}±{train_metrics['cutoff'][3]:.4f}\\n\")\n",
    "        f.write(f\"Valid - PSNR A->B: {valid_metrics['psnr'][0]:.4f}±{valid_metrics['psnr'][1]:.4f}, PSNR B->A: {valid_metrics['psnr'][4]:.4f}±{valid_metrics['psnr'][5]:.4f}\\n\")\n",
    "        f.write(f\"Valid - SSIM A->B: {valid_metrics['ssim'][0]:.4f}±{valid_metrics['ssim'][1]:.4f}, SSIM B->A: {valid_metrics['ssim'][2]:.4f}±{valid_metrics['ssim'][3]:.4f}\\n\")\n",
    "        f.write(f\"Valid - Cutoff A->B: {valid_metrics['cutoff'][0]:.4f}±{valid_metrics['cutoff'][1]:.4f}, Cutoff B->A: {valid_metrics['cutoff'][2]:.4f}±{valid_metrics['cutoff'][3]:.4f}\\n\")\n",
    "\n",
    "def save_models(netG_A2B, netG_B2A, epoch):\n",
    "    torch.save(netG_A2B.state_dict(), f'netG_A2B_epoch_{epoch+1}.pth')\n",
    "    torch.save(netG_B2A.state_dict(), f'netG_B2A_epoch_{epoch+1}.pth')\n",
    "\n",
    "def print_metrics(train_metrics, valid_metrics, epoch, num_epochs):\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}]')\n",
    "    print(f\"Train - PSNR A->B: {train_metrics['psnr'][0]:.4f}±{train_metrics['psnr'][1]:.4f}, PSNR B->A: {train_metrics['psnr'][4]:.4f}±{train_metrics['psnr'][5]:.4f}\")\n",
    "    print(f\"Train - SSIM A->B: {train_metrics['ssim'][0]:.4f}±{train_metrics['ssim'][1]:.4f}, SSIM B->A: {train_metrics['ssim'][2]:.4f}±{train_metrics['ssim'][3]:.4f}\")\n",
    "    print(f\"Train - Cutoff A->B: {train_metrics['cutoff'][0]:.4f}±{train_metrics['cutoff'][1]:.4f}, Cutoff B->A: {train_metrics['cutoff'][2]:.4f}±{train_metrics['cutoff'][3]:.4f}\")\n",
    "    print(f\"Valid - PSNR A->B: {valid_metrics['psnr'][0]:.4f}±{valid_metrics['psnr'][1]:.4f}, PSNR B->A: {valid_metrics['psnr'][4]:.4f}±{valid_metrics['psnr'][5]:.4f}\")\n",
    "    print(f\"Valid - SSIM A->B: {valid_metrics['ssim'][0]:.4f}±{valid_metrics['ssim'][1]:.4f}, SSIM B->A: {valid_metrics['ssim'][2]:.4f}±{valid_metrics['ssim'][3]:.4f}\")\n",
    "    print(f\"Valid - Cutoff A->B: {valid_metrics['cutoff'][0]:.4f}±{valid_metrics['cutoff'][1]:.4f}, Cutoff B->A: {valid_metrics['cutoff'][2]:.4f}±{valid_metrics['cutoff'][3]:.4f}\")\n",
    "\n",
    "# Main execution\n",
    "if __name__ == '__main__':\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Initialize models\n",
    "    netG_A2B = Generator(3, 3).to(device)\n",
    "    netG_B2A = Generator(3, 3).to(device)\n",
    "    netD_A = Discriminator(3).to(device)\n",
    "    netD_B = Discriminator(3).to(device)\n",
    "\n",
    "    # Load datasets\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "    \n",
    "    train_dataset = CycleGANDataset('./data/confocal_exper_altogether_trainR_256.npz', transform=transform)\n",
    "    valid_dataset = CycleGANDataset('./data/confocal_exper_non_sat_filt_validR_256.npz', transform=transform)\n",
    "    valid_paired_dataset = CycleGANDataset('./data/confocal_exper_paired_filt_validsetR_256.npz', transform=transform)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=1, shuffle=False)\n",
    "    valid_paired_loader = DataLoader(valid_paired_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "    # Train the model\n",
    "    num_epochs = 1\n",
    "    train_metrics, valid_metrics, losses = train(netG_A2B, netG_B2A, netD_A, netD_B, train_loader, valid_loader, valid_paired_loader, num_epochs, device)\n",
    "\n",
    "    # Plot metrics\n",
    "    def plot_metrics(train_metrics, valid_metrics, losses, num_epochs):\n",
    "        epochs = range(1, num_epochs + 1)\n",
    "\n",
    "        plt.figure(figsize=(20, 15))\n",
    "        \n",
    "        # PSNR A->B\n",
    "        plt.subplot(331)\n",
    "        plt.errorbar(epochs, [m['psnr'][0] for m in train_metrics], yerr=[m['psnr'][1] for m in train_metrics], label='Train A->B')\n",
    "        plt.errorbar(epochs, [m['psnr'][0] for m in valid_metrics], yerr=[m['psnr'][1] for m in valid_metrics], label='Valid A->B')\n",
    "        plt.title('PSNR A->B')\n",
    "        plt.legend()\n",
    "\n",
    "        # PSNR B->A\n",
    "        plt.subplot(332)\n",
    "        plt.errorbar(epochs, [m['psnr'][4] for m in train_metrics], yerr=[m['psnr'][5] for m in train_metrics], label='Train B->A')\n",
    "        plt.errorbar(epochs, [m['psnr'][4] for m in valid_metrics], yerr=[m['psnr'][5] for m in valid_metrics], label='Valid B->A')\n",
    "        plt.title('PSNR B->A')\n",
    "        plt.legend()\n",
    "\n",
    "        # SSIM A->B\n",
    "        plt.subplot(333)\n",
    "        plt.errorbar(epochs, [m['ssim'][0] for m in train_metrics], yerr=[m['ssim'][1] for m in train_metrics], label='Train A->B')\n",
    "        plt.errorbar(epochs, [m['ssim'][0] for m in valid_metrics], yerr=[m['ssim'][1] for m in valid_metrics], label='Valid A->B')\n",
    "        plt.title('SSIM A->B')\n",
    "        plt.legend()\n",
    "\n",
    "        # SSIM B->A\n",
    "        plt.subplot(334)\n",
    "        plt.errorbar(epochs, [m['ssim'][2] for m in train_metrics], yerr=[m['ssim'][3] for m in train_metrics], label='Train B->A')\n",
    "        plt.errorbar(epochs, [m['ssim'][2] for m in valid_metrics], yerr=[m['ssim'][3] for m in valid_metrics], label='Valid B->A')\n",
    "        plt.title('SSIM B->A')\n",
    "        plt.legend()\n",
    "\n",
    "        # Cutoff Frequency A->B\n",
    "        plt.subplot(335)\n",
    "        plt.errorbar(epochs, [m['cutoff'][0] for m in train_metrics], yerr=[m['cutoff'][1] for m in train_metrics], label='Train A->B')\n",
    "        plt.errorbar(epochs, [m['cutoff'][0] for m in valid_metrics], yerr=[m['cutoff'][1] for m in valid_metrics], label='Valid A->B')\n",
    "        plt.title('Cutoff Frequency A->B')\n",
    "        plt.legend()\n",
    "\n",
    "        # Cutoff Frequency B->A\n",
    "        plt.subplot(336)\n",
    "        plt.errorbar(epochs, [m['cutoff'][2] for m in train_metrics], yerr=[m['cutoff'][3] for m in train_metrics], label='Train B->A')\n",
    "        plt.errorbar(epochs, [m['cutoff'][2] for m in valid_metrics], yerr=[m['cutoff'][3] for m in valid_metrics], label='Valid B->A')\n",
    "        plt.title('Cutoff Frequency B->A')\n",
    "        plt.legend()\n",
    "\n",
    "        # Losses\n",
    "        plt.subplot(337)\n",
    "        plt.plot(epochs, losses['G'], label='Generator')\n",
    "        plt.plot(epochs, losses['D_A'], label='Discriminator A')\n",
    "        plt.plot(epochs, losses['D_B'], label='Discriminator B')\n",
    "        plt.title('Losses')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('metrics_and_losses_plot.png')\n",
    "        plt.close()\n",
    "\n",
    "        print(\"Training completed. Metrics and losses plot saved as 'metrics_and_losses_plot.png'.\")\n",
    "\n",
    "    # Call the plot_metrics function\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "889a004d-d425-471c-b96a-b3201e13b894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inspecting dataset: confocal_exper_altogether_trainR_256.npz\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'confocal_exper_altogether_trainR_256.npz'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 40\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Inspect each dataset\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m dataset_files:\n\u001b[1;32m---> 40\u001b[0m     \u001b[43minspect_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mDataset inspection complete.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[2], line 5\u001b[0m, in \u001b[0;36minspect_dataset\u001b[1;34m(file_path)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minspect_dataset\u001b[39m(file_path):\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mInspecting dataset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m data\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m      8\u001b[0m         array \u001b[38;5;241m=\u001b[39m data[key]\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\numpy\\lib\\npyio.py:427\u001b[0m, in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[0;32m    425\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    426\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 427\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos_fspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    428\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    430\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'confocal_exper_altogether_trainR_256.npz'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def inspect_dataset(file_path):\n",
    "    print(f\"\\nInspecting dataset: {file_path}\")\n",
    "    data = np.load(file_path)\n",
    "    \n",
    "    for key in data.keys():\n",
    "        array = data[key]\n",
    "        print(f\"\\nArray: {key}\")\n",
    "        print(f\"Shape: {array.shape}\")\n",
    "        \n",
    "        if len(array.shape) == 4:\n",
    "            num_samples, channels, height, width = array.shape\n",
    "            print(f\"Number of samples: {num_samples}\")\n",
    "            print(f\"Number of channels: {channels}\")\n",
    "            print(f\"Image dimensions: {height}x{width}\")\n",
    "        elif len(array.shape) == 3:\n",
    "            num_samples, height, width = array.shape\n",
    "            print(f\"Number of samples: {num_samples}\")\n",
    "            print(f\"Number of channels: 1 (grayscale)\")\n",
    "            print(f\"Image dimensions: {height}x{width}\")\n",
    "        else:\n",
    "            print(\"Unexpected array shape\")\n",
    "        \n",
    "        print(f\"Data type: {array.dtype}\")\n",
    "        print(f\"Min value: {array.min()}\")\n",
    "        print(f\"Max value: {array.max()}\")\n",
    "        print(f\"Mean value: {array.mean()}\")\n",
    "        print(f\"Standard deviation: {array.std()}\")\n",
    "\n",
    "# List of dataset files to inspect\n",
    "dataset_files = [\n",
    "    'confocal_exper_altogether_trainR_256.npz',\n",
    "    'confocal_exper_non_sat_filt_validR_256.npz',\n",
    "    'confocal_exper_paired_filt_validsetR_256.npz'\n",
    "]\n",
    "\n",
    "# Inspect each dataset\n",
    "for file in dataset_files:\n",
    "    inspect_dataset(file)\n",
    "\n",
    "print(\"\\nDataset inspection complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8faf5dda-5ca2-432e-b07a-ca18da636b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import save_image\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "def load_model(model_path, device):\n",
    "    model = Generator(3, 3).to(device)\n",
    "    state_dict = torch.load(model_path, map_location=device, weights_only=True)\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def generate_predictions(generator, dataloader, device, output_dir, direction):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (real_A, real_B) in enumerate(dataloader):\n",
    "            if direction == 'A2B':\n",
    "                real = real_A.to(device)\n",
    "                fake = generator(real)\n",
    "            else:  # B2A\n",
    "                real = real_B.to(device)\n",
    "                fake = generator(real)\n",
    "            \n",
    "            # Denormalize images\n",
    "            real = (real + 1) / 2\n",
    "            fake = (fake + 1) / 2\n",
    "            \n",
    "            # Save images\n",
    "            save_image(real, os.path.join(output_dir, f'real60_{direction}_{i}.png'))\n",
    "            save_image(fake, os.path.join(output_dir, f'fake60_{direction}_{i}.png'))\n",
    "            \n",
    "            if i >= 99:  # Generate 100 images\n",
    "                break\n",
    "\n",
    "def main():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Load the trained models\n",
    "    netG_A2B = load_model('./models/netG_A2B_epoch_60.pth', device)  \n",
    "    netG_B2A = load_model('./models/netG_B2A_epoch_60.pth', device)  \n",
    "\n",
    "    # Load the dataset\n",
    "    test_dataset = CycleGANDataset('./data/confocal_exper_paired_filt_validsetR_256.npz', transform=None)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "    # Generate and save predictions\n",
    "    generate_predictions(netG_A2B, test_loader, device, 'predictions_A2B', 'A2B')\n",
    "    generate_predictions(netG_B2A, test_loader, device, 'predictions_B2A', 'B2A')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
